240924_Setup Hadoop Client on Ubuntu 20.04


0.	Prepare

	# Install necessary package

		$ sudo apt install dfault-jdk openjdk-11-jdk-headless -y
        $ sudo apt install openssh-server openssh-client -y
		
	# Edit route file

		Add the following lines to the file:
	
		$ sudo nano /etc/hosts
	
		-----------------------------------------------------------------------
		192.168.0.131 hadoop131
		-----------------------------------------------------------------------
		
		'192.168.0.131' is IP address of namenode


1.	Create `hadoop` account

	# Add a new user `hadoop`
	
		$ sudo adduser hadoop
	
	# Add to the sudo group
	
		$ sudo usermod -aG sudo hadoop
		
	# Switch to `hadoop` account
	
		$ sudo su - hadoop
		
	# Generate RSA keys
	
		$ ssh-keygen -t rsa
		
	# Add public key to authorized keys
	
		$ sudo cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
		$ sudo chmod 640 ~/.ssh/authorized_keys
		
	# Verify ssh connect
	
		$ ssh -p <port> localhost


2.	Install Hadoop
	
	# Download stable version
	
		$ wget https://downloads.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz

	# Extract the downloaded file
	
		$ tar -xvzf hadoop-3.4.0.tar.gz
		
	# Create logs & tmp dir
	
		$ mkdir hadoop-3.4.0/{logs,tmp}
	
	# Move directory
	
		$ sudo mv hadoop-3.4.0 /opt/hadoop-3.4.0
		
	# Change the ownership of hadoop directory
	
		$ sudo chown -R hadoop:hadoop /opt/hadoop-3.4.0
			
			
2.	Setup Environment paths

	# Add hadoop-paths.sh
	
		$ nano ~/hadoop-paths.sh
		
		-------------------------------------------------------------------
		# Hadoop Environment Variables
		export HADOOP_HOME=/opt/hadoop-3.4.0
		export HADOOP_INSTALL=$HADOOP_HOME
		export HADOOP_MAPRED_HOME=$HADOOP_HOME
		export HADOOP_COMMON_HOME=$HADOOP_HOME
		export HADOOP_HDFS_HOME=$HADOOP_HOME
		export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
		export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
		export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib/native"
		export HADOOP_LOG_DIR=$HADOOP_HOME/logs
		export HADOOP_TMP_DIR=$HADOOP_HOME/tmp
		export YARN_HOME=$HADOOP_HOME
		
		export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native
		-------------------------------------------------------------------
		
	# Edir ~/.bashrc
		
		-------------------------------------------------------------------
		...
		
		export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
		source /home/hadoop/hadoop-paths.sh
		-------------------------------------------------------------------
		
		
3.  Edit hadoop-env.sh

	# Add the following lines to the file

		$ nano /opt/hadop-3.4.0/etc/hadoop/hadoop-env.sh
	
		-----------------------------------------------------------
		export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
		export HADOOP_SSH_OPTS="-p <port num>"
		-----------------------------------------------------------
			
		e.g.
		-----------------------------------------------------------
		export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
		export HADOOP_SSH_OPTS="-p 2022"
		-----------------------------------------------------------
		
		
4.	Add NameNode URL
		
	# Add the following lines to the file.

		$ nano /opt/hadoop-3.4.0/etc/hadoop/core-site.xml

	    -------------------------------------------------------------------
        <configuration>
          <property>
            <name>fs.defaultFS</name>
            <value>hdfs://192.168.0.131:9000</value>
          </property>
        </configuration>
	    -------------------------------------------------------------------
		

5.	Test

	# List files of root dir
	
		$ hdfs dfs -ls /
		
	# Get file from HDFS
		
		$ hdfs dfs -get /path/in/hdfs /local/path
		
	# Put file to HDFS
	
		$ hdfs dfs -put /local/in/pathfile /path/to/hdfs
	
	# HDFS shell commands
	
		$ hdfs dfs -help
		
	
	